{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e88267f9-7b72-4fc0-99c1-b4312bde5be3",
      "metadata": {
        "id": "e88267f9-7b72-4fc0-99c1-b4312bde5be3"
      },
      "source": [
        "# BioImage Model Zoo Example notebook\n",
        "\n",
        "This notebook provides examples of how to load pretrained deep learning models from [BioImage Model Zoo](https://bioimage.io) and use them to process new images."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a02b0904",
      "metadata": {
        "id": "a02b0904"
      },
      "source": [
        "## **1. Install key dependencies**\n",
        "---\n",
        "<font size = 4>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61c446cf",
      "metadata": {
        "id": "61c446cf"
      },
      "source": [
        "### **1.0. Install required dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7bfc0d0",
      "metadata": {
        "cellView": "form",
        "id": "b7bfc0d0"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play to install dependencies\n",
        "#@markdown #### DO NOT RESTART THE SESSION UNTIL THE CELL FINISHES RUNNING\n",
        "#@markdown #### This may take few minutes\n",
        "\n",
        "!pip install bioimageio.core==0.6.7\n",
        "!pip install matplotlib==3.9.0\n",
        "!pip install imageio==2.31.2\n",
        "!pip install numpy==1.23.5\n",
        "!pip install torch==2.2.0\n",
        "!pip install pooch==1.8"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "306d47df-e0d3-4856-a534-36aa032a2c83",
      "metadata": {
        "id": "306d47df-e0d3-4856-a534-36aa032a2c83",
        "tags": []
      },
      "source": [
        "### **1.1. Load BioImageIO dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7b1ed61-e608-4403-8c1c-f0398bc31a26",
      "metadata": {
        "cellView": "form",
        "id": "f7b1ed61-e608-4403-8c1c-f0398bc31a26",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play to load the dependencies and functions\n",
        "\n",
        "# BioImage Model Zoo\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# If you'd rather read the warning messages, please comment the follwing two lines.\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from imageio import imread\n",
        "import xarray as xr\n",
        "import numpy as np\n",
        "from bioimageio.core import test_model\n",
        "from bioimageio.core import load_description\n",
        "from bioimageio.core import create_prediction_pipeline\n",
        "from bioimageio.core.prediction import predict, predict_many\n",
        "#from bioimageio.core.resource_io.utils import resolve_source\n",
        "#from bioimageio.core.build_spec import build_model\n",
        "from bioimageio.spec.utils import download\n",
        "import hashlib\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pooch\n",
        "from ruyaml import YAML\n",
        "\n",
        "yaml = YAML(typ=\"safe\")\n",
        "\n",
        "COLLECTION_URL = \"https://raw.githubusercontent.com/bioimage-io/collection-bioimage-io/gh-pages/collection.json\"\n",
        "\n",
        "collection_path = Path(pooch.retrieve(COLLECTION_URL, known_hash=None))\n",
        "\n",
        "with collection_path.open() as f:\n",
        "\n",
        "    collection = json.load(f)\n",
        "\n",
        "model_urls = [entry[\"rdf_source\"] for entry in collection[\"collection\"] if entry[\"type\"] == \"model\"]\n",
        "\n",
        "model_rdfs = [yaml.load(Path(pooch.retrieve(mu, known_hash=None))) for mu in model_urls]\n",
        "\n",
        "pytorch_models = [rdf for rdf in model_rdfs if \"pytorch_state_dict\" in rdf[\"weights\"]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "235264a0",
      "metadata": {
        "id": "235264a0",
        "tags": []
      },
      "source": [
        "## **2. Inspect a model from the BioImage Model Zoo**\n",
        "\n",
        "Here we will guide you through the basic functionalities of the BioImageIO Python package to interact with the content in the BioImage Model Zoo.\n",
        "\n",
        "In the following cell you will get a list of available models in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c96d15d7",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c96d15d7",
        "outputId": "496e9642-bf45-4e4a-9ca0-90c22470b8cb"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Check the models that can be loaded for PyTorch\n",
        "\n",
        "# nickname_list = []\n",
        "print('List of models for PyTorch:\\n')\n",
        "\n",
        "for model in pytorch_models:\n",
        "    # nickname_list.append(model['config']['bioimageio']['nickname'])\n",
        "    print(f\"{model['name']}\\n - {model['config']['bioimageio']['nickname']}\\n - {model['config']['bioimageio']['doi']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5162057-f0d1-419a-a127-2d6cabc5061b",
      "metadata": {
        "id": "f5162057-f0d1-419a-a127-2d6cabc5061b",
        "tags": []
      },
      "source": [
        "### **2.1. Load the resource description specifications of the model**\n",
        "\n",
        "To load a model of your choice, you only need to write one of the following ones in the cell and leave the rest empty.\n",
        "\n",
        "<font size = 3>**`BMZ_MODEL_ID`**: Unique identifier of the model to load in the BioImage Model Zoo, e.g., `impartial-shrimp`. These identifiers are given on each model card in the zoo.\n",
        "\n",
        "OR\n",
        "\n",
        "<font size = 3>**`BMZ_MODEL_DOI`**: Model DOIs can also be used to load the models.\n",
        "\n",
        "OR\n",
        "\n",
        "<font size = 3>**`BMZ_MODEL_URL`**: URL to the main Zenodo repository as well as to the `rdf.yaml` file containing the resource description specifications can be used to load models as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6745eebe-c01e-41ff-918d-41693a5d1526",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6745eebe-c01e-41ff-918d-41693a5d1526",
        "outputId": "4996095f-7ff0-4b58-dea9-ba3878fe0ca1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#@markdown ##Load the model description with one of these options\n",
        "\n",
        "BMZ_MODEL_ID = \"\" #\"affable-shark\" #@param {type:\"string\"}\n",
        "BMZ_MODEL_DOI = \"\" #@param {type:\"string\"}\n",
        "BMZ_MODEL_URL = \"https://uk1s3.embassy.ebi.ac.uk/public-datasets/bioimage.io/affable-shark/draft/files/rdf.yaml\" #@param {type:\"string\"}\n",
        "\n",
        "# Load the model description\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "if BMZ_MODEL_ID != \"\":\n",
        "    model = load_description(BMZ_MODEL_ID)  # TODO: load from bioimageio id\n",
        "    print(f\"The model '{model.name}' with ID '{BMZ_MODEL_ID}' has been correctly loaded.\")\n",
        "elif BMZ_MODEL_DOI != \"\":\n",
        "    model = load_description(BMZ_MODEL_DOI)  # TODO: load from bioimageio id\n",
        "    print(f\"The model '{model.name}' with DOI '{BMZ_MODEL_DOI}' has been correctly loaded.\")\n",
        "elif BMZ_MODEL_URL != \"\":\n",
        "    model = load_description(BMZ_MODEL_URL)  # TODO: load from bioimageio id\n",
        "    print(f\"The model '{model.name}' with URL '{BMZ_MODEL_URL}' has been correctly loaded.\")\n",
        "else:\n",
        "    print('Please specify a model ID, DOI or URL')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07efa2a6-dc1e-46f6-989b-83d95bfaebbf",
      "metadata": {
        "id": "07efa2a6-dc1e-46f6-989b-83d95bfaebbf",
        "tags": []
      },
      "source": [
        "### **2.2. Discover the different components and features of the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZsE2Exb3PEZR",
      "metadata": {
        "id": "ZsE2Exb3PEZR"
      },
      "source": [
        "#### Print information about the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12a35e70",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "12a35e70",
        "outputId": "d2c12619-ba90-4968-902e-7a18e7efc931",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#@markdown ##Print information about the model\n",
        "\n",
        "print(f\"The model '{model.name}' had the following properties and metadata\")\n",
        "print()\n",
        "print(f\" Description {model.description}\")\n",
        "print()\n",
        "print(f\" The authors of the model are {model.authors}\")\n",
        "print(f\" and it is maintained by: {model.maintainers}\")\n",
        "print(f\" License: {model.license}\")\n",
        "print()\n",
        "\n",
        "print(f\" If you use this model, you are expected to cite {model.cite}\")\n",
        "print()\n",
        "print(f\" Further documentation can be found here: {model.cite}\")\n",
        "print()\n",
        "print(f\" GitHub repository: {model.git_repo}\")\n",
        "print()\n",
        "print(f\" Covers of the model '{model.name}' are: \")\n",
        "for cover in model.covers:\n",
        "    cover_data = imread(download(cover).path)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(cover_data)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7_RhSnwTPIL_",
      "metadata": {
        "id": "7_RhSnwTPIL_"
      },
      "source": [
        "#### Inspect the weights and, expected inputs and outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01bc05b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from devtools import pprint\n",
        "import inspect\n",
        "import bioimageio.core\n",
        "\n",
        "\n",
        "\n",
        "a = [[s.min,s.step] if type(s) is bioimageio.spec.model.v0_5.ParameterizedSize else s for s in inp.shape]\n",
        "print(a)\n",
        "out.axes\n",
        "#pprint(dir(model))\n",
        "\n",
        "inp.preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "579ee0e6-fb0f-4c2b-a025-6dfdc5518963",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "579ee0e6-fb0f-4c2b-a025-6dfdc5518963",
        "outputId": "88afd6b5-b751-48c7-d86d-2c4a45a466cb",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#@markdown ##Inspect the weights and, expected inputs and outputs\n",
        "\n",
        "import bioimageio.spec\n",
        "\n",
        "\n",
        "print(\"Available weight formats for this model:\", model.weights.model_fields_set)\n",
        "print(\"Pytorch state dict weights are stored at:\", model.weights.pytorch_state_dict.source.absolute())\n",
        "print()\n",
        "\n",
        "# or what inputs the model expects\n",
        "print(f\"The model requires {len(model.inputs)} input(s) with the following features:\")\n",
        "for inp in model.inputs:\n",
        "    print(\"\\nInput with axes:\", ([i.id for i in inp.axes])) \n",
        "    print(\"Minimum shape:\", ([s.min if type(s) is bioimageio.spec.model.v0_5.ParameterizedSize else s for s in inp.shape]))\n",
        "    print(\"Step:\", ([s.step if type(s) is bioimageio.spec.model.v0_5.ParameterizedSize else s for s in inp.shape]))\n",
        "    print(f\"\\nIt is expected to be processed with: {[[prep.id, prep.kwargs] for prep in inp.preprocessing]}\")\n",
        "print()\n",
        "# and what the model outputs are\n",
        "print(f\"The model gives {len(model.outputs)} output(s) with the following features:\")\n",
        "for out in model.outputs:\n",
        "    print(\"\\nOutput with axes:\", ([o.id for o in out.axes]) )\n",
        "    print(\"Minimum shape:\", ([s if type(s) is bioimageio.spec.model.v0_5.SizeReference else s for s in out.shape]))\n",
        "    print(\"Step:\", ([s.step if type(s) is bioimageio.spec.model.v0_5.ParameterizedSize else s for s in out.shape]))\n",
        "    print(f\"\\nIt is expected to be processed with: {[postp.id for postp in out.postprocessing]}\")\n",
        "    #print(f\"The output image has a halo of : {out.halo}\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Fkz_oROmQNui",
      "metadata": {
        "id": "Fkz_oROmQNui"
      },
      "source": [
        "#### Inspect the test images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00bef6db-a11e-4bf2-bb2b-d2026df35801",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "00bef6db-a11e-4bf2-bb2b-d2026df35801",
        "outputId": "a56b37b0-8495-4fcc-9912-9b2d682e5202",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#@markdown ##Inspect the test images\n",
        "\n",
        "print(f\"The model provides {len(model.inputs)} test input image(s) :\")\n",
        "for test_im in model.get_input_test_arrays():\n",
        "    test_input = np.squeeze(test_im)\n",
        "    if len(test_input.shape)>2:\n",
        "        print(f\"The test input image has shape {test_input.shape}, so it will not displayed\")\n",
        "    else:\n",
        "        plt.figure(figsize=(3,3))\n",
        "        plt.imshow(test_input, cmap=\"afmhot\")\n",
        "        plt.colorbar()\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.show()\n",
        "\n",
        "# Inspect the test output images\n",
        "print(f\"The model provides {len(model.outputs)} test input image(s) :\")\n",
        "for test_im in model.get_output_test_arrays():\n",
        "    test_output = np.squeeze(test_im)\n",
        "    if len(test_output.shape)>2:\n",
        "        print(f\"The test output image has shape {test_output.shape}, so it will not displayed\")\n",
        "    else:\n",
        "        plt.figure(figsize=(3,3))\n",
        "        plt.imshow(test_output, cmap=\"afmhot\")\n",
        "        plt.colorbar()\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5df5f7c-2ce0-4230-b30f-682a6aedf84a",
      "metadata": {
        "id": "f5df5f7c-2ce0-4230-b30f-682a6aedf84a"
      },
      "source": [
        "## **3. Test the model**\n",
        "\n",
        "Both the model format and the deployment of the model can be tested.\n",
        "\n",
        "By running the following cell you can check that\n",
        "- The model follows the format of the BioImage Model Zoo correctly (static validation)\n",
        "- It actually produces the output that is expected to produce (dynamic validation). This is done by running a prediction for the test input images and checking that they agree with the given test output(s).\n",
        "\n",
        "The running time depends on the resources available (e.g., GPU acceleration)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c805d5e7",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c805d5e7",
        "outputId": "8aacc5e5-642e-49b6-9e3f-b2d475e04357",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#@markdown ##Check if the model pass the test\n",
        "\n",
        "# 'test_model' returns a dict with 'status'='passed'/'failed' and more detailed information\n",
        "\n",
        "for test_result in test_model(model):\n",
        "    if test_result[\"status\"] == \"failed\":\n",
        "        print(\"model test:\", test_result[\"name\"])\n",
        "        print(\"The model test failed with:\", test_result[\"error\"])\n",
        "        print(\"with the traceback:\")\n",
        "        print(\"\".join(test_result[\"traceback\"]))\n",
        "    else:\n",
        "        print(\"model test:\", test_result[\"name\"])\n",
        "        test_result[\"status\"] == \"passed\"\n",
        "        print(\"The model passed the test.\")\n",
        "        print()\n",
        "print(f\"The modle was tested using:\")\n",
        "print(f\"'bioimageio_spec_version': '{test_result['bioimageio_spec_version']}'\")\n",
        "print(f\"'bioimageio_core_version': '{test_result['bioimageio_core_version']}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8402e3a5-9093-45f0-8d09-1c2c8b1fdc53",
      "metadata": {
        "id": "8402e3a5-9093-45f0-8d09-1c2c8b1fdc53"
      },
      "source": [
        "## **4. Use the model with new images**\n",
        "\n",
        "The BioImageIO library includes functions to run prediction with `xarray.DataArrays` or on images stored on disc."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eda17596-9fb4-41e8-aef1-e6c4eb2da579",
      "metadata": {
        "id": "eda17596-9fb4-41e8-aef1-e6c4eb2da579",
        "tags": []
      },
      "source": [
        "### **4.1. Process an input array**\n",
        "\n",
        "The prediction pipeline expects inputs as `xarray.DataArrays`, which are similar to `numpy` arrays, but allow for named dimensions (the `dims` keyword argument).\n",
        "In bioimage.io the dimensions have to agree with the input axes required by the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46b37a8a-7ee2-417c-92d9-92a5372e3835",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "46b37a8a-7ee2-417c-92d9-92a5372e3835",
        "outputId": "91522f8e-9dcd-482f-8223-33ab312de8b0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#@markdown ##Process the input within the model\n",
        "\n",
        "# load the example image for this model, which is stored in numpy file format\n",
        "input_image = np.load(model.test_inputs[0])\n",
        "\n",
        "# \"devices\" can be used to run prediction on a gpu instead of the cpu\n",
        "devices = None\n",
        "# \"weight_format\" to specify which weight format to use in case the model contains different weight formats\n",
        "weight_format = None\n",
        "# the prediction pipeline combines preprocessing, prediction and postprocessing.\n",
        "# it should always be used for prediction with a bioimageio model\n",
        "pred_pipeline = create_prediction_pipeline(\n",
        "    bioimageio_model=model, devices=devices, weight_format=weight_format\n",
        ")\n",
        "# the prediction pipeline expects inputs as xarray.DataArrays.\n",
        "axes = tuple(model.inputs[0].axes)\n",
        "input_tensor = xr.DataArray(input_image, dims=axes)\n",
        "\n",
        "# the prediction pipeline call expects the same number of inputs as the number of inputs required by the model\n",
        "# in the case here, the model just expects a single input. in the case of multiple inputs use\n",
        "# prediction = pred_pipeline(input1, input2, ...)\n",
        "# or, if you have the inputs in a list or tuple\n",
        "# prediction = pred_pipeline(*inputs)\n",
        "# the call returns a list of output tensors, corresponding to the output tensors of the model\n",
        "# (in this case, we just have a single output)\n",
        "prediction = pred_pipeline(input_tensor)[0]\n",
        "\n",
        "prediction = np.squeeze(prediction)\n",
        "\n",
        "if len(prediction.shape)>2:\n",
        "    subplot_n = prediction.shape[0]\n",
        "    plt.figure(figsize=(15,10))\n",
        "    plt.subplot(1,1+subplot_n,1)\n",
        "    plt.imshow(np.squeeze(input_image), cmap=\"afmhot\")\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Input image to process\")\n",
        "\n",
        "    for i in range(subplot_n):\n",
        "        plt.subplot(1,1+subplot_n,i+2)\n",
        "        plt.imshow(np.squeeze(prediction[i]), cmap=\"afmhot\")\n",
        "        plt.axis('off')\n",
        "        plt.title(\"Processed image\")\n",
        "    plt.show()\n",
        "else:\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.imshow(np.squeeze(input_image), cmap=\"afmhot\")\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Input image to process\")\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.imshow(np.squeeze(prediction), cmap=\"afmhot\")\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Processed image\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43854569-1e3a-443c-9e5e-fc2b97f26480",
      "metadata": {
        "cellView": "form",
        "id": "43854569-1e3a-443c-9e5e-fc2b97f26480",
        "tags": []
      },
      "source": [
        "### **4.2. Process an image from a directory and save the result**\n",
        "\n",
        "The BioImageIO is equipped with the utility function `predict` to run prediction with an image stored on disc. The filepath where the output should be stored (`save_outputs`), supports most common image formats (`.tif`, `.png`) as well as `npy` fileformat.\n",
        "\n",
        "Provide the path to the image to be processes in `path2image` or run it with the example image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40230c43-711e-43b0-b816-c7c5640a36f1",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "40230c43-711e-43b0-b816-c7c5640a36f1",
        "outputId": "7fbf7db8-fd56-4d98-e7b3-efe24b0155a8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#@markdown ##Indicate the path to the image\n",
        "\n",
        "# path + name of the stored output\n",
        "use_test_image = True #@param {type:\"boolean\"}\n",
        "#@markdown ### If you have an image in a folder to segment, copy the path to it here:\n",
        "path2image = \"\"  #@param {type:\"string\"}\n",
        "#@markdown ### Indicate where to save the output of the model:s\n",
        "save_outputs = \"/content/prediction.tif\"  #@param {type:\"string\"}\n",
        "\n",
        "if use_test_image:\n",
        "  path2image = model.test_inputs\n",
        "\n",
        "# model resource, the image that will be process, path to store the output\n",
        "predict(model, path2image, [save_outputs])\n",
        "\n",
        "# the output tensor contains 2 channels, which is not supported by normal tif.\n",
        "# thus, these 2 channels are stored as 2 separate images\n",
        "\n",
        "\n",
        "## Check the number of channels in the ouput and needed images to read\n",
        "for test_im in model.test_outputs:\n",
        "    test_output = np.squeeze(np.load(test_im))\n",
        "    if len(test_output.shape)>2:\n",
        "      fg_pred = [f\"{save_outputs[:-4]}-c{i}.{save_outputs[-3:]}\" for i in range(test_output.shape[0])]\n",
        "    else:\n",
        "      fg_pred = [save_outputs]\n",
        "fg_pred = [imread(f) for f in fg_pred]\n",
        "\n",
        "if use_test_image:\n",
        "  input_image = np.load(model.test_inputs[0])\n",
        "else:\n",
        "  input_image = np.squeeze(imread(path2image))\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(1,1+len(fg_pred),1)\n",
        "plt.imshow(np.squeeze(input_image), cmap=\"afmhot\")\n",
        "plt.axis('off')\n",
        "plt.title(\"Input image to process\")\n",
        "k=2\n",
        "for f in fg_pred:\n",
        "  plt.subplot(1,1+len(fg_pred),k)\n",
        "  plt.imshow(f, cmap=\"afmhot\")\n",
        "  plt.axis('off')\n",
        "  plt.title(\"Processed image\")\n",
        "  k+=1\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2c9cf63-56e6-4f15-9ae6-bfdc5f99620f",
      "metadata": {
        "id": "e2c9cf63-56e6-4f15-9ae6-bfdc5f99620f",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "### **4.3. Process a set of images stored in a directory**\n",
        "It is possible to provide a list of images to analyse and run the analysis automatically. In this analysis `padding` and `tiling` strategies are enabled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "774b95c7-3ab5-4c58-a6c9-d07179caf036",
      "metadata": {
        "cellView": "form",
        "id": "774b95c7-3ab5-4c58-a6c9-d07179caf036",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#@markdown ##Indicate a directory with images to analyse and a directory to save the images\n",
        "\n",
        "im_dir = \"\" #@param {type:\"string\"}\n",
        "results_dir = \"\" #@param {type:\"string\"}\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# Get the list of images to analyse and the same list to save the images\n",
        "inputs = [os.path.join(im_dir,f) for f in os.listdir(im_dir) if not f.startswith('.')]\n",
        "outputs = [os.path.join(results_dir, os.path.split(inp)[1]) for inp in inputs]\n",
        "\n",
        "print(len(inputs), \"images for prediction were found\")\n",
        "\n",
        "predict(model, inputs, outputs, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c655f09b-f04a-4def-9b1e-fa98c25faa2b",
      "metadata": {
        "cellView": "form",
        "id": "c655f09b-f04a-4def-9b1e-fa98c25faa2b"
      },
      "outputs": [],
      "source": [
        "#@markdown ## If the images are too big we can even choose a padding.\n",
        "\n",
        "padding_x = 16 #@param {type:\"integer\"}\n",
        "padding_y = 16 #@param {type:\"integer\"}\n",
        "\n",
        "padding = {\"x\": padding_x, \"y\": padding_y, \"mode\": \"dynamic\"}\n",
        "predict(model, inputs, outputs, padding=padding, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "343e5f1d-db1a-4d1c-842b-bd77ed7f7fb0",
      "metadata": {
        "cellView": "form",
        "id": "343e5f1d-db1a-4d1c-842b-bd77ed7f7fb0"
      },
      "outputs": [],
      "source": [
        "#@markdown ## Instead of padding, you can also use tiling.\n",
        "\n",
        "# here, we specify a tile size of 224 and a halo (= extension of tile on both sides)\n",
        "# size of 16, which results in an effective tile shale of 256 = 224 + 2*16\n",
        "\n",
        "tile_x = 224 #@param {type:\"integer\"}\n",
        "tile_y = 224 #@param {type:\"integer\"}\n",
        "halo_x = 16 #@param {type:\"integer\"}\n",
        "halo_y = 16 #@param {type:\"integer\"}\n",
        "\n",
        "tiling = {\n",
        "    \"tile\": {\"x\": tile_x, \"y\": tile_y},\n",
        "    \"halo\": {\"x\": halo_x, \"y\": halo_y},\n",
        "}\n",
        "\n",
        "predict_many(model, inputs, outputs, tiling=tiling, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UnYCJehOiSSF",
      "metadata": {
        "id": "UnYCJehOiSSF"
      },
      "source": [
        "## **5. Fine-tune an existing model (only for segmentation)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IY-b0yMcNOV8",
      "metadata": {
        "id": "IY-b0yMcNOV8"
      },
      "source": [
        "### **5.1. Connect to your google drive to access training data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ec91lO2QM0ks",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec91lO2QM0ks",
        "outputId": "121a5819-a707-42ea-abc5-a2b820532af0"
      },
      "outputs": [],
      "source": [
        "#@markdown ## Run this cell to connect google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UM-fC1nwNQr2",
      "metadata": {
        "id": "UM-fC1nwNQr2"
      },
      "source": [
        "### **5.2. Start the fine-tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kfz2a6oQRuLT",
      "metadata": {
        "id": "kfz2a6oQRuLT"
      },
      "source": [
        "##### Run the following cell to load the training functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5iczcnVViOJt",
      "metadata": {
        "cellView": "form",
        "id": "5iczcnVViOJt"
      },
      "outputs": [],
      "source": [
        "#@markdown ## Load the training functions\n",
        "\n",
        "from marshmallow import missing\n",
        "import torch\n",
        "import tifffile\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MultiLabelSoftMarginLoss\n",
        "from torch.nn.functional import one_hot\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "def load_pytorch_model(model):\n",
        "    weight_spec = model.weights.get(\"pytorch_state_dict\")\n",
        "    model_kwargs = weight_spec.kwargs\n",
        "    joined_kwargs = {} if model_kwargs is missing else dict(model_kwargs)\n",
        "    model_instance = weight_spec.architecture(**joined_kwargs)\n",
        "    _devices = [\"cuda\" if torch.cuda.is_available() else \"cpu\"]\n",
        "\n",
        "    print(_devices)\n",
        "    if len(_devices) > 1:\n",
        "        warnings.warn(\"Multiple devices for single pytorch model not yet implemented\")\n",
        "    model_instance.to(_devices[0])\n",
        "    weights = model.weights.get(\"pytorch_state_dict\")\n",
        "    if weights is not None and weights.source:\n",
        "        state = torch.load(weights.source, map_location=_devices[0])\n",
        "        model_instance.load_state_dict(state)\n",
        "    model_instance.eval()\n",
        "\n",
        "    return model_instance\n",
        "\n",
        "\n",
        "class SegmentationTrainDataset(Dataset):\n",
        "    def __init__(self,  INPUT_IMAGE_WIDTH, INPUT_IMAGE_HEIGHT, imagePaths, maskPaths, classes):\n",
        "        # store the image and mask filepaths, and augmentation\n",
        "        # transforms\n",
        "        self.imagePaths = imagePaths\n",
        "        self.maskPaths = maskPaths\n",
        "        self.classes = classes\n",
        "    def __len__(self):\n",
        "        # return the number of total samples contained in the dataset\n",
        "        return len(self.imagePaths)\n",
        "\n",
        "    def preprocess(self, im):\n",
        "        norm_im = np.float32(im)\n",
        "        return (norm_im - np.mean(norm_im)) / (np.std(norm_im) + 1.0e-6)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab the image path from the current index\n",
        "        image = tifffile.imread(self.imagePaths[idx])\n",
        "        mask = tifffile.imread(self.maskPaths[idx])\n",
        "        # we want to ensure that there are cells in the patch\n",
        "        # without getting in an infinite loop\n",
        "        # TODO: define a sampling function to remove the loop\n",
        "        num_labels = 0\n",
        "        k = 0\n",
        "        while num_labels<(self.classes-1) and k<5:\n",
        "          # Choose a random coordinate to crop a patch\n",
        "          h = random.randint(1, image.shape[0]-INPUT_IMAGE_HEIGHT-1)\n",
        "          w = random.randint(1, image.shape[1]-INPUT_IMAGE_WIDTH-1)\n",
        "          mask_patch = mask[h:h+INPUT_IMAGE_HEIGHT, w:w+INPUT_IMAGE_WIDTH]\n",
        "          num_labels = len(np.unique(mask_patch))\n",
        "          # If the mask contains more than one label for semantic segmentation\n",
        "          # we will trasnform into one-hot encoding\n",
        "          mask_torch = torch.tensor(mask_patch).to(torch.int64)\n",
        "          mask_hot = one_hot(mask_torch, self.classes)\n",
        "          mask_hot = mask_hot[:,:,1:]\n",
        "          if len(mask_hot.shape)==2:\n",
        "            # add a dimension\n",
        "            mask_hot = np.expand_dims(mask_hot, -1)\n",
        "          # first axis goes to the channels\n",
        "          mask_hot = np.transpose(mask_hot, [-1, 0, 1])\n",
        "          k += 1\n",
        "          # return a tuple of the image and its mask\n",
        "        norm_image = self.preprocess(image)\n",
        "        norm_image = np.expand_dims(norm_image[h:h+INPUT_IMAGE_HEIGHT, w:w+INPUT_IMAGE_WIDTH], 0)\n",
        "        return (torch.tensor(norm_image).float(), torch.tensor(mask_hot).float())\n",
        "\n",
        "class SegmentationTestDataset(Dataset):\n",
        "    def __init__(self,  INPUT_IMAGE_WIDTH, INPUT_IMAGE_HEIGHT, imagePaths, maskPaths, classes):\n",
        "        # store the image and mask filepaths, and augmentation\n",
        "        # transforms\n",
        "        self.imagePaths = imagePaths\n",
        "        self.maskPaths = maskPaths\n",
        "        self.classes = classes\n",
        "    def __len__(self):\n",
        "        # return the number of total samples contained in the dataset\n",
        "        return len(self.imagePaths)\n",
        "\n",
        "    def preprocess(self, im):\n",
        "        norm_im = np.float32(im)\n",
        "        return (norm_im - np.mean(norm_im)) / (np.std(norm_im) + 1.0e-6)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab the image path from the current index\n",
        "        image = tifffile.imread(self.imagePaths[idx])\n",
        "        mask = tifffile.imread(self.maskPaths[idx])\n",
        "        # no patches are cropped. Check for the memory\n",
        "        mask_torch = torch.tensor(mask).to(torch.int64)\n",
        "        mask_hot = one_hot(mask_torch, self.classes)\n",
        "        mask_hot = mask_hot[:,:,1:]\n",
        "        if len(mask_hot.shape)==2:\n",
        "          # add a dimension\n",
        "          mask_hot = np.expand_dims(mask_hot, -1)\n",
        "        # first axis goes to the channels\n",
        "        mask_hot = np.transpose(mask_hot, [-1, 0, 1])\n",
        "        # return a tuple of the image and its mask\n",
        "        norm_image = self.preprocess(image)\n",
        "        norm_image = np.expand_dims(norm_image, 0)\n",
        "        return (torch.tensor(norm_image).float(), torch.tensor(mask_hot).float())\n",
        "\n",
        "\n",
        "def visualize_results(input_image, gt, prediction):\n",
        "  if len(prediction.shape)>2:\n",
        "      subplot_n = prediction.shape[0]\n",
        "      plt.figure(figsize=(20,15))\n",
        "      plt.subplot(1,1+2*subplot_n,1)\n",
        "      plt.imshow(np.squeeze(input_image), cmap=\"gray\")\n",
        "      plt.axis('off')\n",
        "      plt.title(\"Input test image\")\n",
        "\n",
        "      for i in range(subplot_n):\n",
        "          plt.subplot(1,1+2*subplot_n,i+2)\n",
        "          plt.imshow(np.squeeze(gt[i]), cmap=\"afmhot\")\n",
        "          plt.axis('off')\n",
        "          plt.title(\"Ground truth image\")\n",
        "\n",
        "          plt.subplot(1,1+2*subplot_n,i+4)\n",
        "          plt.imshow(np.squeeze(prediction[i]), cmap=\"afmhot\")\n",
        "          plt.axis('off')\n",
        "          plt.title(\"Processed image\")\n",
        "      plt.show()\n",
        "  else:\n",
        "      plt.figure(figsize=(10,10))\n",
        "      plt.subplot(1,3,1)\n",
        "      plt.imshow(np.squeeze(input_image), cmap=\"afmhot\")\n",
        "      plt.axis('off')\n",
        "      plt.title(\"Input test image\")\n",
        "      plt.subplot(1,3,2)\n",
        "      plt.imshow(np.squeeze(gt), cmap=\"afmhot\")\n",
        "      plt.axis('off')\n",
        "      plt.title(\"Ground truth image\")\n",
        "\n",
        "      plt.subplot(1,3,3)\n",
        "      plt.imshow(np.squeeze(prediction), cmap=\"afmhot\")\n",
        "      plt.axis('off')\n",
        "      plt.title(\"Processed image\")\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "def finetune_bioimageio_model(model, TRAIN_IM, TRAIN_MASK, TEST_IM, TEST_MASK,\n",
        "                              BASE_OUTPUT, NUM_EPOCHS=100, INIT_LR=0.0001, BATCH_SIZE=10,\n",
        "                              INPUT_IMAGE_WIDTH=512, INPUT_IMAGE_HEIGHT=512, CLASSES=3):\n",
        "\n",
        "    model_instance = load_pytorch_model(model)\n",
        "    # create the train and test datasets\n",
        "    trainImages = sorted([os.path.join(TRAIN_IM, i) for i in os.listdir(TRAIN_IM) if i.endswith(\".tif\") ])\n",
        "    trainMasks = sorted([os.path.join(TRAIN_MASK, i) for i in os.listdir(TRAIN_MASK) if i.endswith(\".tif\") ])\n",
        "    testImages = sorted([os.path.join(TEST_IM, i) for i in os.listdir(TEST_IM) if i.endswith(\".tif\") ])\n",
        "    testMasks = sorted([os.path.join(TEST_MASK, i) for i in os.listdir(TEST_MASK) if i.endswith(\".tif\") ])\n",
        "\n",
        "    trainDS = SegmentationTrainDataset( INPUT_IMAGE_WIDTH, INPUT_IMAGE_HEIGHT, imagePaths=trainImages, maskPaths=trainMasks, classes = CLASSES)\n",
        "    testDS = SegmentationTestDataset( INPUT_IMAGE_WIDTH, INPUT_IMAGE_HEIGHT, imagePaths=testImages, maskPaths=testMasks, classes = CLASSES)\n",
        "    print(f\"[INFO] found {len(trainDS)} examples in the training set...\")\n",
        "    print(f\"[INFO] found {len(testDS)} examples in the test set...\")\n",
        "\n",
        "    # create the training and test data loaders\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "    # determine the device to be used for training and evaluation\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    # determine if we will be pinning memory during data loading\n",
        "    PIN_MEMORY = True if DEVICE == \"cuda\" else False\n",
        "    trainLoader = DataLoader(trainDS, shuffle=True, batch_size=BATCH_SIZE, pin_memory=PIN_MEMORY, num_workers=os.cpu_count())\n",
        "    testLoader = DataLoader(testDS, shuffle=False, batch_size=1, pin_memory=PIN_MEMORY, num_workers=os.cpu_count())\n",
        "\n",
        "    # initialize loss function and optimizer\n",
        "    lossFunc = CrossEntropyLoss()\n",
        "    opt = Adam(model_instance.parameters(), lr=INIT_LR)\n",
        "    # calculate steps per epoch for training and test set\n",
        "    trainSteps = len(trainDS) // BATCH_SIZE\n",
        "    testSteps = len(testDS)\n",
        "    # initialize a dictionary to store training history\n",
        "    H = {\"train_loss\": [], \"test_loss\": []}\n",
        "\n",
        "    # Save the initial prediction\n",
        "    x, y = testDS.__getitem__(-1)\n",
        "    x = x.unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        # set the model in evaluation mode\n",
        "        model_instance.eval()\n",
        "        # send the input to the device\n",
        "        (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
        "        # make the predictions\n",
        "        pred = model_instance(x)\n",
        "\n",
        "    input_image = x.to(\"cpu\")\n",
        "    input_image = np.squeeze(input_image.numpy())\n",
        "\n",
        "    gt = y.to(\"cpu\")\n",
        "    gt = np.squeeze(gt.numpy())\n",
        "\n",
        "    prediction = pred.to(\"cpu\")\n",
        "    prediction = np.squeeze(prediction.numpy())\n",
        "    print(\"Results of the prediction before finetuning\")\n",
        "    print(\"---------------------------------------------\")\n",
        "    visualize_results(input_image, gt, prediction)\n",
        "    print(\"---------------------------------------------\")\n",
        "    del x, y, gt, prediction, input_image, pred\n",
        "\n",
        "    # loop over epochs\n",
        "    print(\"[INFO] training the network...\")\n",
        "    startTime = time.time()\n",
        "    for e in tqdm(range(NUM_EPOCHS)):\n",
        "        # set the model in training mode\n",
        "        model_instance.train()\n",
        "        # initialize the total training and validation loss\n",
        "        totalTrainLoss = 0\n",
        "        totalTestLoss = 0\n",
        "        # loop over the training set\n",
        "\n",
        "        for (i, (x, y)) in enumerate(trainLoader):\n",
        "            # send the input to the device\n",
        "            (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
        "            # perform a forward pass and calculate the training loss\n",
        "            pred = model_instance(x)\n",
        "            loss = lossFunc(pred, y)\n",
        "            # first, zero out any previously accumulated gradients, then\n",
        "            # perform backpropagation, and then update model parameters\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            # add the loss to the total training loss so far\n",
        "            totalTrainLoss += loss\n",
        "        # switch off autograd\n",
        "        with torch.no_grad():\n",
        "            # set the model in evaluation mode\n",
        "            model_instance.eval()\n",
        "            # loop over the validation set\n",
        "            for (x, y) in testLoader:\n",
        "                # send the input to the device\n",
        "                (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
        "                # make the predictions and calculate the validation loss\n",
        "                pred = model_instance(x)\n",
        "                totalTestLoss += lossFunc(pred, y)\n",
        "        if DEVICE==\"cuda\":\n",
        "            torch.save(model_instance.state_dict(),\n",
        "                       os.path.join(BASE_OUTPUT, \"finetuned_last.pth\"))\n",
        "        else:\n",
        "            torch.save(model_instance.cpu().state_dict(),\n",
        "                       os.path.join(BASE_OUTPUT, \"finetuned_last.pth\"))\n",
        "\n",
        "        # calculate the average training and validation loss\n",
        "        avgTrainLoss = totalTrainLoss / trainSteps\n",
        "        avgTestLoss = totalTestLoss / testSteps\n",
        "        # update our training history\n",
        "        H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
        "        H[\"test_loss\"].append(avgTestLoss.cpu().detach().numpy())\n",
        "        # print the model training and validation information\n",
        "        print(\"[INFO] EPOCH: {}/{}\".format(e + 1, NUM_EPOCHS))\n",
        "        print(\"Train loss: {:.6f}, Test loss: {:.4f}\".format(\n",
        "            avgTrainLoss, avgTestLoss))\n",
        "    # display the total time needed to perform the training\n",
        "    endTime = time.time()\n",
        "    print(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))\n",
        "\n",
        "    input_image = x.to(\"cpu\")\n",
        "    input_image = np.squeeze(input_image.numpy())\n",
        "\n",
        "    gt = y.to(\"cpu\")\n",
        "    gt = np.squeeze(gt.numpy())\n",
        "\n",
        "    prediction = pred.to(\"cpu\")\n",
        "    prediction = np.squeeze(prediction.numpy())\n",
        "\n",
        "    visualize_results(input_image, gt, prediction)\n",
        "\n",
        "    return model_instance, H\n",
        "\n",
        "# get the python file defining the architecture.\n",
        "# this is only required for models with pytorch_state_dict weights\n",
        "def get_architecture_source(rdf):\n",
        "    import bioimageio\n",
        "    # here, we need the raw resource, which contains the information from the resource description\n",
        "    # before evaluation, e.g. the file and name of the python file with the model architecture\n",
        "    raw_resource = bioimageio.core.load_raw_resource_description(rdf)\n",
        "    # the python file defining the architecture for the pytorch weihgts\n",
        "    model_source = raw_resource.weights[\"pytorch_state_dict\"].architecture\n",
        "    # download the source file if necessary\n",
        "    source_file = bioimageio.core.resource_io.utils.resolve_source(\n",
        "        model_source.source_file\n",
        "    )\n",
        "    # if the source file path does not exist, try combining it with the root path of the model\n",
        "    if not os.path.exists(source_file):\n",
        "        source_file = os.path.join(raw_resource.root_path, os.path.split(source_file)[1])\n",
        "    assert os.path.exists(source_file), source_file\n",
        "    class_name = model_source.callable_name\n",
        "    return f\"{source_file}:{class_name}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RjcVR1RYR1N1",
      "metadata": {
        "id": "RjcVR1RYR1N1"
      },
      "source": [
        "##### Run the following cell to visualize the results of the pretrained model on the new images before running the fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X7hNpKpAL7lg",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "X7hNpKpAL7lg",
        "outputId": "2d03cdf8-873b-4018-c14f-fa49b1378e9b"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Visualise the output of the model on an example image before fine-tuning.\n",
        "\n",
        "path2image = \"/content/Multilabel_U-Net_dataset_B.subtilis/test/source/test_9.tif\"  #@param {type:\"string\"}\n",
        "#@markdown ### Indicate where to save the output of the model:s\n",
        "save_outputs = \"/content/prediction.tif\"  #@param {type:\"string\"}\n",
        "\n",
        "# model resource, the image that will be process, path to store the output\n",
        "predict(model, path2image, [save_outputs])\n",
        "\n",
        "# the output tensor contains 2 channels, which is not supported by normal tif.\n",
        "# thus, these 2 channels are stored as 2 separate images\n",
        "\n",
        "\n",
        "## Check the number of channels in the ouput and needed images to read\n",
        "for test_im in model.test_outputs:\n",
        "    test_output = np.squeeze(np.load(test_im))\n",
        "    if len(test_output.shape)>2:\n",
        "      fg_pred = [f\"{save_outputs[:-4]}-c{i}.{save_outputs[-3:]}\" for i in range(test_output.shape[0])]\n",
        "    else:\n",
        "      fg_pred = [save_outputs]\n",
        "fg_pred = [imread(f) for f in fg_pred]\n",
        "\n",
        "input_image = np.squeeze(imread(path2image))\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(1,1+len(fg_pred),1)\n",
        "plt.imshow(np.squeeze(input_image), cmap=\"afmhot\")\n",
        "plt.title(\"Input image to process\")\n",
        "k=2\n",
        "for f in fg_pred:\n",
        "  plt.subplot(1,1+len(fg_pred),k)\n",
        "  plt.imshow(f, cmap=\"afmhot\")\n",
        "  plt.axis('off')\n",
        "  plt.title(\"Processed image\")\n",
        "  k+=1\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Kj8oT2FCR-7W",
      "metadata": {
        "id": "Kj8oT2FCR-7W"
      },
      "source": [
        "##### Run the following cell to set up the parameters for the fine-tuning and start it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1OT8OBediZeK",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1OT8OBediZeK",
        "outputId": "65721b94-e5ed-440d-85b3-b6c6055cd72c"
      },
      "outputs": [],
      "source": [
        "#@markdown ## Set up training parameters\n",
        "\n",
        "\n",
        "# define the number of channels in the input, number of classes,\n",
        "# and number of levels in the U-Net model\n",
        "\n",
        "# initialize learning rate, number of epochs to train for, and the\n",
        "# batch size\n",
        "INIT_LR = 0.000001 #@param {type:\"number\"}\n",
        "NUM_EPOCHS = 150 #@param {type:\"integer\"}\n",
        "BATCH_SIZE = 20 #@param {type:\"integer\"}\n",
        "# define the input image dimensions\n",
        "INPUT_IMAGE_WIDTH = 256 #@param {type:\"integer\"}\n",
        "INPUT_IMAGE_HEIGHT = 256 #@param {type:\"integer\"}\n",
        "CLASSES=3\n",
        "# Training data files\n",
        "TRAIN_IM = \"/content/drive/Multilabel_U-Net_dataset_B.subtilis/training/source\" #@param {type:\"string\"}\n",
        "TRAIN_MASK = \"/content/Multilabel_U-Net_dataset_B.subtilis/training/target_boundaries\" #@param {type:\"string\"}\n",
        "# Test data files\n",
        "TEST_IM = \"/content/Multilabel_U-Net_dataset_B.subtilis/test/source\" #@param {type:\"string\"}\n",
        "TEST_MASK = \"/content/Multilabel_U-Net_dataset_B.subtilis/test/target_boundaries\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "#@markdown ### Path to the directory to save the new model\n",
        "\n",
        "# define the path to the base output directory\n",
        "BASE_OUTPUT = \"/content/bioimageio_finetuning/\" #@param {type:\"string\"}\n",
        "os.makedirs(BASE_OUTPUT, exist_ok=True)\n",
        "# define the path to the output serialized model, model training\n",
        "# plot, and testing image paths\n",
        "MODEL_PATH = os.path.join(BASE_OUTPUT, \"finetuned_bioimageio.pth\")\n",
        "PLOT_PATH = os.path.sep.join([BASE_OUTPUT, \"plot.png\"])\n",
        "\n",
        "#-----\n",
        "#del model_instance\n",
        "#del finetuned_model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "#gc.collect()\n",
        "\n",
        "\n",
        "finetuned_model, H = finetune_bioimageio_model(model, TRAIN_IM, TRAIN_MASK, TEST_IM, TEST_MASK,\n",
        "                                          BASE_OUTPUT, NUM_EPOCHS=NUM_EPOCHS, INIT_LR=INIT_LR, BATCH_SIZE=BATCH_SIZE,\n",
        "                                          INPUT_IMAGE_WIDTH=INPUT_IMAGE_WIDTH, INPUT_IMAGE_HEIGHT=INPUT_IMAGE_HEIGHT,\n",
        "                                               CLASSES=3)\n",
        "\n",
        "# Save the model in two different formats\n",
        "MODEL_STETEDICT_PATH = os.path.join(BASE_OUTPUT, \"finetuned_bioimageio_statedict_model.pth\")\n",
        "torch.save(finetuned_model.cpu().state_dict(),MODEL_STETEDICT_PATH)\n",
        "## Use the following to convert the model into torchscript\n",
        "MODEL_TORCHSCRIPT_PATH = os.path.join(BASE_OUTPUT, \"finetuned_bioimageio_torchscript_model.pt\")\n",
        "with torch.no_grad():\n",
        "    # load input and expected output data\n",
        "    input_data = [np.load(inp).astype(\"float32\") for inp in model.test_inputs]\n",
        "    input_data = [torch.from_numpy(inp) for inp in input_data]\n",
        "    scripted_model = torch.jit.trace(finetuned_model.cpu(), input_data)\n",
        "    scripted_model.save(MODEL_TORCHSCRIPT_PATH)\n",
        "\n",
        "# plot the training loss\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
        "plt.plot(H[\"test_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training Loss on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(PLOT_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d56d6ebf",
      "metadata": {
        "id": "d56d6ebf"
      },
      "source": [
        "## 6. Create a BioImage Model Zoo model\n",
        "\n",
        "Let's recreate a model based on parts of the loaded model description from above!\n",
        "\n",
        "`bioimageio.core` also implements functionality to create a model package compatible with the [BioImnageIO Model Spec](https://bioimage.io/docs/#/bioimageio_model_spec) ready to be shared via the [Bioimage Model Zoo](https://bioimage.io/#/).\n",
        "Here, we will use this functionality to create a new model with the finetuned weights.\n",
        "\n",
        "For this we are using some information from the previouse model.\n",
        "\n",
        "Run the following cell to export the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45b01287",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45b01287",
        "outputId": "09970f2b-c815-4ff7-b0df-18f6b97bc38f",
        "tags": []
      },
      "outputs": [],
      "source": [
        "## TODO\n",
        "# ------\n",
        "# information about the model\n",
        "\n",
        "#@markdown ##Export the new model to the bioimage model zoo format\n",
        "\n",
        "Trained_model_name = \"My new model\" #@param {type:\"string\"}\n",
        "Trained_model_description = \"Model finetuned\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Choose a test image\n",
        "path2image = \"/content/drive/Multilabel_U-Net_dataset_B.subtilis/test/source/test_9.tif\"  #@param {type:\"string\"}\n",
        "training_data_bioimnageio_id = \"zero/dataset_u-net_2d_multilabel_deepbacs\"  #@param {type:\"string\"}\n",
        "# create the training data\n",
        "training_data = {\"id\": training_data_bioimnageio_id}\n",
        "#training_data = {\"source\": training_data_source,\n",
        "#                  \"description\": training_data_description}\n",
        "\n",
        "#@markdown ### Path where the new model checkpoint is saved\n",
        "BASE_OUTPUT = \"/content/bioimageio_finetuning/\" #@param {type:\"string\"}\n",
        "MODEL_TORCHSCRIPT_PATH = os.path.join(BASE_OUTPUT, \"finetuned_bioimageio_torchscript_model.pt\")\n",
        "output_path = os.path.join(BASE_OUTPUT, \"finetuned_bioimageio_model.zip\")\n",
        "\n",
        "authors = [{\"name\": model.authors[0].name, \"affiliation\": model.authors[0]}]\n",
        "Trained_model_license = model.license\n",
        "readme_path = model.documentation\n",
        "citations = [{'text': i.text, 'url': i.url} for i in model.cite]\n",
        "\n",
        "\n",
        "axes = ''\n",
        "for i in model.inputs[0].axes:\n",
        "  axes = axes+i\n",
        "preprocessing = [[{\"name\": prep.name, \"kwargs\": prep.kwargs} for prep in inp.preprocessing] for inp in model.inputs]\n",
        "kwargs = dict(\n",
        "  input_names = [model.inputs[0].name],\n",
        "  input_data_range = [[-np.inf, np.inf]],\n",
        "  input_axes = [axes],\n",
        "  input_min_shape = [model.inputs[0].shape.min],\n",
        "  input_step =[ model.inputs[0].shape.step],\n",
        "  preprocessing =  preprocessing)\n",
        "\n",
        "axes = ''\n",
        "for i in model.outputs[0].axes:\n",
        "  axes = axes+i\n",
        "# There is no postprocessing here\n",
        "#postprocessing = [[{\"name\": post.name, \"kwargs\": post.kwargs} for post in inp.postprocessing] for inp in model.outputs]\n",
        "\n",
        "output_spec = dict(\n",
        "  output_names = [model.outputs[0].name],\n",
        "  output_data_range =  [[-np.inf, np.inf]],\n",
        "  output_axes = [axes],\n",
        "  #postprocessing = postprocessing,\n",
        "  output_reference =[ model.outputs[0].shape.reference_tensor],\n",
        "  output_scale = [model.outputs[0].shape.scale], # consider changing it if the input has more than one channel\n",
        "  output_offset =[ model.outputs[0].shape.offset]\n",
        ")\n",
        "kwargs.update(output_spec)\n",
        "\n",
        "\n",
        "new_input_path = f\"{BASE_OUTPUT}/new_test_input.npy\"\n",
        "test_im = imread(path2image)\n",
        "test_im = test_im[-256:, :256]\n",
        "test_im = np.expand_dims(test_im, [0,1])\n",
        "np.save(new_input_path, test_im)\n",
        "\n",
        "new_output_path = f\"{BASE_OUTPUT}/new_test_output.npy\"\n",
        "np.save(new_output_path, np.float32(test_im))\n",
        "\n",
        "# model_source = get_architecture_source(\"affable-shark\")\n",
        "\n",
        "# we create the model, process the input image and create the model again with the correct output.\n",
        "for i in range(2):\n",
        "  build_model(\n",
        "      name = Trained_model_name,\n",
        "      description = Trained_model_description,\n",
        "      # additional metadata about authors, licenses, citation etc.\n",
        "      authors = authors,\n",
        "      license = Trained_model_license,\n",
        "      documentation = readme_path,\n",
        "      # the weight file and the type of the weights\n",
        "      weight_uri = MODEL_TORCHSCRIPT_PATH,\n",
        "      weight_type = \"torchscript\",\n",
        "      # the test input and output data as well as the description of the tensors\n",
        "      # these are passed as list because we support multiple inputs / outputs per model\n",
        "      test_inputs = [new_input_path],\n",
        "      test_outputs =  [new_output_path],\n",
        "      # where to save the model zip, how to call the model and a short description of it\n",
        "      output_path = output_path,\n",
        "      tags=[\"in-silico-labeling\",\"pytorch\", \"cyclegan\", \"conditional-gan\",\n",
        "            \"zerocostdl4mic\", \"deepimagej\", \"actin\", \"dapi\", \"cells\", \"nuclei\",\n",
        "            \"fluorescence-light-microscopy\", \"2d\"],  # the tags are used to make models more findable on the website\n",
        "      cite = citations,\n",
        "      training_data = training_data,\n",
        "      add_deepimagej_config=True,\n",
        "      **kwargs\n",
        "      )\n",
        "  if i == 0:\n",
        "    predict(model_rdf = output_path, inputs = new_input_path, outputs = new_output_path)\n",
        "\n",
        "# check that the model works for keras and tensorflow\n",
        "res = test_model(output_path, weight_format=\"torchscript\")\n",
        "success = True\n",
        "if res[-1][\"error\"] is not None:\n",
        "  success = False\n",
        "  print(\"test-model failed:\", res[-1][\"error\"])\n",
        "\n",
        "if success:\n",
        "  print(\"The bioimage.io model was successfully exported to\", output_path)\n",
        "else:\n",
        "  print(\"The bioimage.io model was exported to\", output_path)\n",
        "  print(\"Some tests of the model did not work!l.\")\n",
        "  print(\"You can still download and test the model, but it may not work as expected.\")\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
